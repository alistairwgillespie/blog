{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \"Architecture of a Neural Network\"\n",
        "> \"A high-level breakdown of a neural network.\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: false\n",
        "- tags: true\n",
        "- image: images/copied_from_nb/architecture_imgs/architecture.png\n",
        "- comments: true\n",
        "- categories: [deeplearning]\n",
        "- author: Alistair Gillespie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's time to progress on to the architecture of a neural network. By way of doing this we will address some deep learning jargon. At a high level there are three main components to a generic neural network: the input layer made up of input neurons, the output layer made up of output neurons, and the hidden layer. The hidden layer is literally any layer that is not an input or output layer. It's funny, for a while I wasn't sure what a hidden layer meant. Does a hidden layer get it's name due to some mathematical significance? The answer is no. Like I said before, it's just a layer that is neither an input or output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Architecture of Neural Network](architecture_imgs/architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Let's go ahead and introduce another hidden layer to our architecture. Now that we have an additional layer some folks might refer to this architecture as a multilayer perceptron. The reason this name is used is largerly due to the history of neural networks. This terminology is used despite the fact that most architectures use sigmoid neurons instead of perceptrons - it's misleading to say the least. An example of this can be found in this [article](https://www.microsoft.com/security/blog/2021/07/27/combing-through-the-fuzz-using-fuzzy-hashing-and-deep-learning-to-counter-malware-detection-evasion-techniques/ \"Combing through the fuzz\") by Microsoft on applying multilayer perceptrons to polymorphic malware classification. I find it hard to believe that the temperamental perceptron is doing the heavylifting in this case, but I could be wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Architecture of Neural Network](architecture_imgs/mlp.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of layers and neurons is defined based on the task. For example, if we want to classify a 256 x 256 grayscale images of cats, we would define an input later of 65,536 input neurons (256 x 256). Each neuron then represents the intensity of each corresponding pixel from the image - scaled between 0 and 1. The output layer, containing a single output neuron, would produce a value between 0 and 1, with values above 0.5 indicating the image contains a cat, and values less than 0.5 indicating the image doesn't contain a cat. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another important property of this network is that each layer of neurons provides inputs to the next layer in the network. This property is called feedforward; in other words, there are no loops in the network. So there we have it, a quick introduction to feedforward neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This post was developed with the help of: \n",
        "<iframe src=\"https://open.spotify.com/embed/track/3ZdVayrMwJEzi99uOss8he?utm_source=generator\" width=\"100%\" height=\"380\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\"></iframe>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2021-11-22-Perceptrons.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
