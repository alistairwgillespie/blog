{
  
    
        "post0": {
            "title": "Sigmoid Neurons",
            "content": "Perceptrons got a lot of folks excited back in the day. However, there are was a bit of a problem. Say we are trying to classify digits correctly using a network of perceptrons. Our model takes an input image of the digit &quot;1&quot;, and incorrectly classifies it as a &quot;2&quot;. Why don&#39;t we just go ahead and tweak the weights and bias to improve our model&#39;s classification? Well, we can, but by doing so we get unpredictable results. For example, say we do tune the parameters and correctly predict &quot;1&quot; in the next training run, but now our model is performing poorly on other digits. We&#39;ve essentially kicked the can down the road, and our model begins to juggle between different weights and biases with no significant gains in accuracy across all digits. . This sensitivity to change quickly becomes unwieldy when tackling machine learning tasks, even the slightest change to the parameters will produce a completely different output. Instead, we need a model that gives us greater control of the output so we are able to fine tune the model to achieve better performance. In other words, for any small change in the weights and bias, we only want a small change in the output. . With this simple idea, the sigmoid neuron was born. The math can be kinda scary at first but some generous folks have broken it down into edible chunks for us. I&#39;ll summarise for you. The sigmoid has similar qualities to a perceptron but a few key differences. Like it&#39;s sibling, the sigmoid neuron still takes inputs $x_1, x_2, ..., x_j$, however $x_j$ can be any value between $0$ and $1$ (e.g. 0.7456). Weights are still assigned to each input $w_1, w_2, ..., w_3$, and the overall bias remains. However, the output is no longer a 0 or 1, instead it&#39;s any number between 0 and 1 depending on the magnitude of $w.x + b$. This is because of the $ sigma$ being applied to get $ sigma(w.x +b)$. So what does the sigmoid part of it actually do? It does this: $$ sigma(z) equiv frac{1}{1+e^{-z}}$$ . Before we simplify things, let&#39;s go another step further to make things more explicit. The output of a sigmoid neuron with the inputs, weights and bias detailed above is: $$ frac{1}{1 + exp(- sum_jw_jx_j-b)}$$ . Wowza. It&#39;s starting to look pretty funky. Despite our function looking more complex, the sigmoid neuron still possesses similarities to the perceptron. For instance, say $z$, or $w.x+b$ is a big number - $x^{-z}=2.718281828^{-100}=0$. Finishing the calculation, and the output is 1. So when $w.x + b$ is large and positive, the output will be approximately 1. On the other hand, if the input is large and negative, the output will be approximately 0. This is awfully similar to the perceptron, right? The sigmoid differentiates itself when z lands in the between these two extremes with subtle tweaks to the weights and biases producing small corresponding updates to the output. . This post was developed with the help of: .",
            "url": "http://alistairgillespie.io/deeplearning/2021/11/25/Sigmoid-Neurons.html",
            "relUrl": "/deeplearning/2021/11/25/Sigmoid-Neurons.html",
            "date": " • Nov 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Perceptrons",
            "content": "Let&#39;s start with one of the earliest forms of machine learning, the perceptron. It make&#39;s sense to start here given modern day neural networks evolved from this point. The perceptron was inspired by neurons in the brain; the idea being that a neuron&#39;s functionality is represented using a function that takes a set of inputs and generates an output. So how does it work? Let&#39;s visualise it first: . . Here we can see a perceptron takes a set of binary inputs, $x_1,x_2,...,x_j$, and multiplies each input by an assigned weight $w_1, w_2 text{,...,} w_j$. Each input&#39;s weight is based on importance; the larger the weight, the greater the importance of the input. To represent this algebraically, we get the following: . $$ y = sum_j w_jx_j$$ . If the output, $y$, is greater than the defined threshold, the output will be $1$, otherwise it will be $0$. So let&#39;s extend our equation to represent this: . $$ output = begin{cases} 1 &amp; text{if} &amp; sum_j w_jx_j gt text{threshold} 0 &amp; text{if} &amp; sum_j w_jx_j le text{threshold} end{cases} $$ I think at this juncture we can cement this knowledge with an example. Let&#39;s use a perceptron as a decision making model for deciding whether we go watch the West Coast Eagles versus North Melbourne Kangaroos. Our model will take in three binary inputs: . Is it raining or not? | Are any friends or family available to join us? | Is the opponent a competitive side (i.e. top 8)? | I don&#39;t like heading to the footy in the rain, in light of this we assign a weight of 6 to #1. Moreover, I like to have company when I head to the footy, so I assign a weight of 4 to #2. Lastly, I prefer to go to the important games against top 8 sides so we assign a weight of 2. We now have the following weights: $w_1=6, w_2=4, w_3=2$. For my decision threshold, I will assign a 5, in other words, if my model output is greater than 5 I will attend the footy. . We are just informed that the weather is going to be sunny, however no friends or family are available to join me. And well, North Melbourne aren&#39;t the side they used to be, sitting last on the ladder. This provides the following inputs to my model: $x_1 = 1, x_2=0, x_3=0$. Let&#39;s go ahead and calculate the output: . $$ 6*1 + 4*0 + 2*0 = 6$$ . 6 is greater than my threshold, we get a 1 as output, and I&#39;m heading to the footy! The model clearly appreciates sunny weather and the availability of company, if neither are true, it&#39;s likely this model will suggest I stay home. I&#39;m a little more passionate about the Eagles than this example let&#39;s on but you get the gist. It&#39;s important to note that by changing the weights and threshold, we will get different models of decision making. . Perceptrons provide a building block for thinking about and building multi-layer networks. Like a web of lego, we can leverage perceptrons to build multi-layered models, like so: . . The above example illustrates an initial layer taking a set of inputs and producing a set of n decisions or outputs (4 in this case). The subsequent layer then takes those outputs as inputs to determine a final decision. We can think of the first layer as making a set of simple decisions, which then allows the next layer to conduct more subtle or complex decisions. This layered approach can be extended both by the number of layers and nodes to achieve complex and sophisticated decision making models. . To finish, let&#39;s put the final touches on how we represent the perceptron mathematically. We will move the threshold to the other side of the inequality and introduce the bias ($b$), then lastly we will exchange the sum for the dot product - enter linear algebra. This prepares us nicely for some of the notation coming up in deep learning. . $$ output = begin{cases} 1 &amp; text{if} &amp; w.x + b gt 0 0 &amp; text{if} &amp; w.x + b le 0 end{cases} $$ This post was developed with the help of: .",
            "url": "http://alistairgillespie.io/deeplearning/2021/11/22/Perceptrons.html",
            "relUrl": "/deeplearning/2021/11/22/Perceptrons.html",
            "date": " • Nov 22, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a Senior Manager and Data Scientist working for EY Cyber Security. I hold a double Bachelor of Computer Science and Commerce, majoring in Computation and Web Technologies, from UWA. . I started my career building websites and as founding CTO of Mandarin Minds where I led the development of an elearning platform. I then went on to establish my own digital media business, Howling Web, where I designed and built bespoke websites for a range of businesses. Then in 2015 I joined EY and made a home for myself in the Cyber Security practice working across a range of projects for some of Australia’s leading corporations including: . Defining a three year strategy to establish a world class security operations center at a major utility. | Developing a deep learning system to augment the threat detection and response capability at a big four bank. | Developing a scalable and automated user access review system to reduce indentity risk in core systems at a big four bank. | Deploying a security operations log management platfrom and coordinating large-scale log ingestion activities to improve detection coverage and situational awareness at a major utility. | Conducting threat modeling exercises to prioritise security uplift and focus areas at a major utility. | Developing a security controls measurement framework to support continuous risk management efforts at a major airline. | . Education . Bachelor of Computer Science, University of Western Australia. 2014 Double majored in Computation and Web Technologies | . | Bachelor of Commerce, University of Western Australia. 2014 Majored in Corporate Finance | . | GMON Continuous Monitoring, GIAC. 2019 | Certificate in NLP with Deep Learning, Stanford Center for Professional Development. 2020 | Machine Learning Nanodegree, Udacity. 2020 | . Work Experience . Senior Manager and Data Scientist, Cyber Security Analytics, EY. 2015 - Present | Founder &amp; Web Developer, Howling Web. 2013 - 2016 | Co-founder &amp; CTO, Mandarin Minds. 2013 - 2019 | Web Developer, Dash Digital. 2013 | . Media . Machine Learning and Cyber Security - Detecting malicious URLs in the haystack, PyCon AU. 2019 | .",
          "url": "http://alistairgillespie.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://alistairgillespie.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}